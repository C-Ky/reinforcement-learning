{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cdcb57",
   "metadata": {},
   "source": [
    "# RL6 - Notebook Actor-Critic Algorithms\n",
    "\n",
    "By Hedwin BONNAVAUD - hedwin.bonnavaud@isae.fr\n",
    "\n",
    " 1. [Description](#description)\n",
    " 2. [Prerequisites](#prerequisites)\n",
    " 2. [Actor-critic](#actor_critic)\n",
    "     1. [Deep Deterministic policy gradient](#ddpg)\n",
    "     2. [Soft-Actor Critic](#sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a1e8b",
   "metadata": {},
   "source": [
    "## Description\n",
    "<div id=\"description\"></div>\n",
    "\n",
    "In the reinforcement basics notebook, you saw how an actor critic architecture can be used to learn an optimal policy in a given environment. This architecture is also mandatory when we want to do reinforcement learning in a continuous actions action space environment, and we are going to see why latter. Our goal in this notebook, is to implement two of the most famous actor-critic algorithms, DDPG and SAC, to solve continuous action space environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d29ae",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93991838",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Environments\n",
    "\n",
    "First, let's introduce the environments we are going to use.\n",
    "\n",
    "#### [Pendulum-v0](https://gym.openai.com/envs/Pendulum-v0/)\n",
    "\n",
    "Here, the agent controll the force applied on the pendulum. The pendulum cannot move, but it can rotate aroud it's axis. His rotation speed depend on the force applied by the agent.\n",
    "\n",
    "The state of the cos and sin of the rotating part angle, and his angle.\n",
    "\n",
    "The action space is obviously continuous. In other work, this is a continuous and simpler (even if in general, continuous mean harder) version on the swing up environment we saw at the last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a91ec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pie-rl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: Box(1,)\n",
      "observation space: Box(3,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pendulum_environment = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(\"action space:\", pendulum_environment.action_space)\n",
    "print(\"observation space:\", pendulum_environment.observation_space)\n",
    "\n",
    "observation = pendulum_environment.reset()\n",
    "pendulum_environment.render()\n",
    "for i in range(100):\n",
    "    _, _, done, _ = pendulum_environment.step(pendulum_environment.action_space.sample())\n",
    "    pendulum_environment.render()\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.01)\n",
    "\n",
    "\n",
    "pendulum_environment.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1ebe",
   "metadata": {},
   "source": [
    "#### [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2/)\n",
    "\n",
    "In this environment, our agent is a lunar lander space ship, that is trying to land on the moon on a specific place.\n",
    "\n",
    "The state space is here a bit complex, and contain informations about agent situation, like his angle, his rotation velocity, his horizontal and vertical velocity, his geographical position, ... The latest is a really important information because the landing pad is located at coordinate [0, 0].\n",
    "    \n",
    "The availables actions is the amount of gaz send in 3 directions (left, right, and bottom) and the more the agent send gaz in a direction, the more he will move in the opposite way. Actions made are visible on the random agent video that you will build by running the next cell.\n",
    "\n",
    "Note: left and right gaz are controlled by the same action, so the agent have an action space made of two continuous actions (click on the link in the environment name above for more information).\n",
    "\n",
    "If you want to play, you can find many other environments on [OpenAI gym](https://gym.openai.com/envs/#classic_control) website. Don't forget to check the documentation and look deeper into the source code to know if they have a continuous or discrete action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df019f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: Box(2,)\n",
      "observation space: Box(8,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pendulum_environment = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "print(\"action space:\", pendulum_environment.action_space)\n",
    "print(\"observation space:\", pendulum_environment.observation_space)\n",
    "\n",
    "observation = pendulum_environment.reset()\n",
    "pendulum_environment.render()\n",
    "for i in range(100):\n",
    "    _, _, done, _ = pendulum_environment.step(pendulum_environment.action_space.sample())\n",
    "    pendulum_environment.render()\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.01)\n",
    "\n",
    "pendulum_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb278307",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "To simplify the global code of this notebook, I propose you tho use a mother class for each of our agents. This will allow us to build a simulation function that take in argument an environment and an agent, and work with any agents and any environments. Then all you will have to do is to describe what the agent should do at each step of the learning.\n",
    "\n",
    "I give you here this class, with the replay buffer from RL5 notebook, and an implementation of DQN that follow ou main Agent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c7b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device=DEVICE):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.device = device\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(x).to(self.device), list(zip(*batch))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    An global agent class that describe the interactions between our agent and it's environment\n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, device=DEVICE, name=\"base_agent\"):\n",
    "\n",
    "        self.name = name  # The name is used inside plot legend, outputs directory path, and outputs file names\n",
    "\n",
    "        self.state_space = state_space\n",
    "        self.state_shape = state_space.shape\n",
    "        self.state_size = state_space.shape[0]  # Assume state space is continuous\n",
    "\n",
    "        self.continuous = isinstance(action_space, gym.spaces.Box)\n",
    "        self.action_space = action_space\n",
    "        self.nb_actions = self.action_space.shape[0] if self.continuous else self.action_space.n\n",
    "        self.last_state = None  # Usefull to store interaction when we recieve (new_stare, reward, done) tuple\n",
    "        self.device = device\n",
    "        self.episode_id = 0\n",
    "        self.episode_time_step_id = 0\n",
    "        self.time_step_id = 0\n",
    "    \n",
    "    def on_simulation_start(self):\n",
    "        \"\"\"\n",
    "        Called when an episode is started. will be used by child class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def on_episode_start(self, state):\n",
    "        self.last_state = state\n",
    "        self.episode_time_step_id = 0\n",
    "        self.episode_id = 0\n",
    "\n",
    "    def action(self, state):\n",
    "        res = self.action_space.sample()\n",
    "        return res\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.episode_time_step_id += 1\n",
    "        self.time_step_id += 1\n",
    "        self.last_state = new_state\n",
    "\n",
    "    def on_episode_stop(self):\n",
    "        self.episode_id += 1\n",
    "\n",
    "    def on_simulation_stop(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Now we can define our DQN using Agent class\n",
    "class DQNAgent(Agent):\n",
    "    \"\"\"\n",
    "    An agent that learn an approximated Q-Function using a neural network. \n",
    "    This Q-Function is used to find the best action to execute in a given state. \n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, name=\"DQN\",\n",
    "                gamma=0.95, epsilon_min=0.01, epsilon_max=1., epsilon_decay_period=1000, epsilon_decay_delay=20,\n",
    "                buffer_size=1000000, learning_rate=0.001, update_target_freq=100, batch_size=20,\n",
    "                layer_1_size=50, layer_2_size=50, nb_gradient_steps=1):\n",
    "        \n",
    "        assert isinstance(action_space, gym.spaces.Discrete)  # Make sure our action space is discrete\n",
    "        super().__init__(state_space, action_space, name=name)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_decay_delay = epsilon_decay_delay\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.nb_gradient_steps = nb_gradient_steps\n",
    "        \n",
    "        self.epsilon_step = (epsilon_max - self.epsilon_min) / epsilon_decay_period\n",
    "        self.total_steps = 0\n",
    "        self.model = torch.nn.Sequential(nn.Linear(self.state_size, layer_1_size),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(layer_1_size, layer_2_size),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(layer_2_size, self.nb_actions)).to(self.device)\n",
    "        \n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.target_model = copy.deepcopy(self.model).to(self.device)\n",
    "        self.update_target_freq = update_target_freq\n",
    "    \n",
    "    def on_simulation_start(self):\n",
    "        self.epsilon = self.epsilon_max\n",
    "    \n",
    "    def action(self, state):\n",
    "        if self.time_step_id > self.epsilon_decay_delay:                \n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_step)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:  # Epsilon greedy\n",
    "            action = np.random.randint(self.nb_actions)\n",
    "        else:\n",
    "            # greedy_action(self.model, state) function in RL5 notebook\n",
    "            with torch.no_grad():\n",
    "                Q = self.model(torch.Tensor(state).unsqueeze(0).to(self.device))\n",
    "                action = torch.argmax(Q).item()\n",
    "        return action\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
    "        self.learn()\n",
    "        super().on_action_stop(action, new_state, reward, done)  # Repale self.last_state by the new_state\n",
    "    def learn(self):\n",
    "        for _ in range(self.nb_gradient_steps):\n",
    "            # gradient_step() function in RL5 notebook\n",
    "            if len(self.replay_buffer) > self.batch_size:\n",
    "                states, actions, rewards, new_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "                Q_new_state_max = self.target_model(new_states).max(1)[0].detach()\n",
    "                update = torch.addcmul(rewards, self.gamma, 1 - dones, Q_new_state_max)\n",
    "                Q_s_a = self.model(states).gather(1, actions.to(torch.long).unsqueeze(1))\n",
    "                loss = self.criterion(Q_s_a, update.unsqueeze(1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        # update target network if needed\n",
    "        if self.time_step_id % self.update_target_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c4ae9",
   "metadata": {},
   "source": [
    "Now let's build a function that run a simulation where a given agent interact with a givent environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a34d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(environment, agent, nb_episodes=200, verbose=True):\n",
    "    episodes_rewards_sum = []\n",
    "    agent.on_simulation_start()\n",
    "    for episode_id in range(nb_episodes):\n",
    "        state = environment.reset()\n",
    "        agent.on_episode_start(state)\n",
    "\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.action(state)\n",
    "            state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            # Ending time step process ...\n",
    "            agent.on_action_stop(action, state, reward, done)\n",
    "\n",
    "            # Store reward\n",
    "            episode_rewards.append(reward)\n",
    "        agent.on_episode_stop()\n",
    "        rewards_sum = sum(episode_rewards)\n",
    "        episodes_rewards_sum.append(rewards_sum)\n",
    "        environment.close()\n",
    "        if len(episodes_rewards_sum) > 20:\n",
    "            last_20_average = mean(episodes_rewards_sum[-20:])\n",
    "        else:\n",
    "            last_20_average = mean(episodes_rewards_sum)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Episode \", '{:3d}'.format(episode_id),\n",
    "                  \", episode return \", '{:4.1f}'.format(rewards_sum),\n",
    "                  \", last 20 avg \", '{:4.1f}'.format(last_20_average),\n",
    "                  sep='')\n",
    "    return episodes_rewards_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333f066",
   "metadata": {},
   "source": [
    "Our agent convergence speed and average performances, will depend on many randomness.\n",
    "To make sure it work and to have a better estimation of it's perfomances, we should run many seeds, many simulations, and observe the average sum of rewards and it's standard deviation over these simulations.\n",
    "\n",
    "This algorithm is also given to you because it's not reinforcement learning and not interesting for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7857968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pie-rl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/tmp/ipykernel_5179/443534885.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return list(map(lambda x:torch.Tensor(x).to(self.device), list(zip(*batch))))\n",
      "/tmp/ipykernel_5179/443534885.py:142: UserWarning: This overload of addcmul is deprecated:\n",
      "\taddcmul(Tensor input, Number value, Tensor tensor1, Tensor tensor2, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul(Tensor input, Tensor tensor1, Tensor tensor2, *, Number value, Tensor out) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  update = torch.addcmul(rewards, self.gamma, 1 - dones, Q_new_state_max)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   0, episode return 18.0, last 20 avg 18.0\n",
      "Episode   1, episode return 15.0, last 20 avg 16.5\n",
      "Episode   2, episode return 17.0, last 20 avg 16.7\n",
      "Episode   3, episode return 70.0, last 20 avg 30.0\n",
      "Episode   4, episode return 42.0, last 20 avg 32.4\n",
      "Episode   5, episode return 19.0, last 20 avg 30.2\n",
      "Episode   6, episode return 28.0, last 20 avg 29.9\n",
      "Episode   7, episode return 10.0, last 20 avg 27.4\n",
      "Episode   8, episode return 32.0, last 20 avg 27.9\n",
      "Episode   9, episode return 12.0, last 20 avg 26.3\n",
      "Episode  10, episode return 12.0, last 20 avg 25.0\n",
      "Episode  11, episode return 11.0, last 20 avg 23.8\n",
      "Episode  12, episode return 13.0, last 20 avg 23.0\n",
      "Episode  13, episode return 12.0, last 20 avg 22.2\n",
      "Episode  14, episode return 20.0, last 20 avg 22.1\n",
      "Episode  15, episode return 11.0, last 20 avg 21.4\n",
      "Episode  16, episode return 10.0, last 20 avg 20.7\n",
      "Episode  17, episode return 10.0, last 20 avg 20.1\n",
      "Episode  18, episode return 11.0, last 20 avg 19.6\n",
      "Episode  19, episode return 13.0, last 20 avg 19.3\n",
      "Episode  20, episode return 10.0, last 20 avg 18.9\n",
      "Episode  21, episode return 15.0, last 20 avg 18.9\n",
      "Episode  22, episode return 25.0, last 20 avg 19.3\n",
      "Episode  23, episode return 12.0, last 20 avg 16.4\n",
      "Episode  24, episode return 14.0, last 20 avg 15.0\n",
      "Episode  25, episode return 23.0, last 20 avg 15.2\n",
      "Episode  26, episode return 12.0, last 20 avg 14.4\n",
      "Episode  27, episode return 12.0, last 20 avg 14.5\n",
      "Episode  28, episode return 11.0, last 20 avg 13.4\n",
      "Episode  29, episode return 13.0, last 20 avg 13.5\n",
      "Episode  30, episode return 10.0, last 20 avg 13.4\n",
      "Episode  31, episode return 13.0, last 20 avg 13.5\n",
      "Episode  32, episode return 13.0, last 20 avg 13.5\n",
      "Episode  33, episode return 10.0, last 20 avg 13.4\n",
      "Episode  34, episode return 13.0, last 20 avg 13.1\n",
      "Episode  35, episode return 13.0, last 20 avg 13.2\n",
      "Episode  36, episode return 11.0, last 20 avg 13.2\n",
      "Episode  37, episode return 13.0, last 20 avg 13.3\n",
      "Episode  38, episode return 10.0, last 20 avg 13.3\n",
      "Episode  39, episode return  9.0, last 20 avg 13.1\n",
      "Episode  40, episode return 10.0, last 20 avg 13.1\n",
      "Episode  41, episode return 14.0, last 20 avg 13.1\n",
      "Episode  42, episode return 10.0, last 20 avg 12.3\n",
      "Episode  43, episode return 12.0, last 20 avg 12.3\n",
      "Episode  44, episode return 11.0, last 20 avg 12.2\n",
      "Episode  45, episode return 15.0, last 20 avg 11.8\n",
      "Episode  46, episode return 18.0, last 20 avg 12.1\n",
      "Episode  47, episode return 12.0, last 20 avg 12.1\n",
      "Episode  48, episode return 14.0, last 20 avg 12.2\n",
      "Episode  49, episode return 19.0, last 20 avg 12.5\n",
      "Episode  50, episode return 14.0, last 20 avg 12.7\n",
      "Episode  51, episode return 23.0, last 20 avg 13.2\n",
      "Episode  52, episode return 15.0, last 20 avg 13.3\n",
      "Episode  53, episode return 15.0, last 20 avg 13.6\n",
      "Episode  54, episode return 11.0, last 20 avg 13.4\n",
      "Episode  55, episode return 22.0, last 20 avg 13.9\n",
      "Episode  56, episode return 16.0, last 20 avg 14.2\n",
      "Episode  57, episode return 16.0, last 20 avg 14.3\n",
      "Episode  58, episode return 25.0, last 20 avg 15.1\n",
      "Episode  59, episode return 20.0, last 20 avg 15.6\n",
      "Episode  60, episode return 18.0, last 20 avg 16.0\n",
      "Episode  61, episode return 20.0, last 20 avg 16.3\n",
      "Episode  62, episode return 25.0, last 20 avg 17.1\n",
      "Episode  63, episode return 24.0, last 20 avg 17.6\n",
      "Episode  64, episode return 20.0, last 20 avg 18.1\n",
      "Episode  65, episode return 18.0, last 20 avg 18.2\n",
      "Episode  66, episode return 25.0, last 20 avg 18.6\n",
      "Episode  67, episode return 26.0, last 20 avg 19.3\n",
      "Episode  68, episode return 29.0, last 20 avg 20.1\n",
      "Episode  69, episode return 46.0, last 20 avg 21.4\n",
      "Episode  70, episode return 20.0, last 20 avg 21.7\n",
      "Episode  71, episode return 25.0, last 20 avg 21.8\n",
      "Episode  72, episode return 32.0, last 20 avg 22.6\n",
      "Episode  73, episode return 24.0, last 20 avg 23.1\n",
      "Episode  74, episode return 37.0, last 20 avg 24.4\n",
      "Episode  75, episode return 23.0, last 20 avg 24.4\n",
      "Episode  76, episode return 18.0, last 20 avg 24.6\n",
      "Episode  77, episode return 29.0, last 20 avg 25.2\n",
      "Episode  78, episode return 104.0, last 20 avg 29.1\n",
      "Episode  79, episode return 101.0, last 20 avg 33.2\n",
      "Episode  80, episode return 107.0, last 20 avg 37.6\n",
      "Episode  81, episode return 178.0, last 20 avg 45.5\n",
      "Episode  82, episode return 185.0, last 20 avg 53.5\n",
      "Episode  83, episode return 85.0, last 20 avg 56.6\n",
      "Episode  84, episode return 140.0, last 20 avg 62.6\n",
      "Episode  85, episode return 149.0, last 20 avg 69.2\n",
      "Episode  86, episode return 160.0, last 20 avg 75.9\n",
      "Episode  87, episode return 300.0, last 20 avg 89.6\n",
      "Episode  88, episode return 149.0, last 20 avg 95.6\n",
      "Episode  89, episode return 113.0, last 20 avg 99.0\n",
      "Episode  90, episode return 116.0, last 20 avg 103.8\n",
      "Episode  91, episode return 165.0, last 20 avg 110.8\n",
      "Episode  92, episode return 196.0, last 20 avg 119.0\n",
      "Episode  93, episode return 167.0, last 20 avg 126.1\n",
      "Episode  94, episode return 160.0, last 20 avg 132.2\n",
      "Episode  95, episode return 119.0, last 20 avg 137.1\n",
      "Episode  96, episode return 153.0, last 20 avg 143.8\n",
      "Episode  97, episode return 137.0, last 20 avg 149.2\n",
      "Episode  98, episode return 126.0, last 20 avg 150.3\n",
      "Episode  99, episode return 116.0, last 20 avg 151.1\n",
      "Episode 100, episode return 151.0, last 20 avg 153.2\n",
      "Episode 101, episode return 148.0, last 20 avg 151.8\n",
      "Episode 102, episode return 215.0, last 20 avg 153.2\n",
      "Episode 103, episode return 187.0, last 20 avg 158.3\n",
      "Episode 104, episode return 161.0, last 20 avg 159.4\n",
      "Episode 105, episode return 166.0, last 20 avg 160.2\n",
      "Episode 106, episode return 145.0, last 20 avg 159.5\n",
      "Episode 107, episode return 175.0, last 20 avg 153.2\n",
      "Episode 108, episode return 208.0, last 20 avg 156.2\n",
      "Episode 109, episode return 171.0, last 20 avg 159.1\n",
      "Episode 110, episode return 157.0, last 20 avg 161.2\n",
      "Episode 111, episode return 138.0, last 20 avg 159.8\n",
      "Episode 112, episode return 141.0, last 20 avg 157.1\n",
      "Episode 113, episode return 152.0, last 20 avg 156.3\n",
      "Episode 114, episode return 153.0, last 20 avg 155.9\n",
      "Episode 115, episode return 156.0, last 20 avg 157.8\n",
      "Episode 116, episode return 147.0, last 20 avg 157.5\n",
      "Episode 117, episode return 147.0, last 20 avg 158.0\n",
      "Episode 118, episode return 153.0, last 20 avg 159.3\n",
      "Episode 119, episode return 152.0, last 20 avg 161.2\n",
      "Episode 120, episode return 172.0, last 20 avg 162.2\n",
      "Episode 121, episode return 160.0, last 20 avg 162.8\n",
      "Episode 122, episode return 166.0, last 20 avg 160.3\n",
      "Episode 123, episode return 141.0, last 20 avg 158.1\n",
      "Episode 124, episode return 137.0, last 20 avg 156.8\n",
      "Episode 125, episode return 124.0, last 20 avg 154.8\n",
      "Episode 126, episode return 125.0, last 20 avg 153.8\n",
      "Episode 127, episode return 152.0, last 20 avg 152.6\n",
      "Episode 128, episode return 135.0, last 20 avg 148.9\n",
      "Episode 129, episode return 114.0, last 20 avg 146.1\n",
      "Episode 130, episode return 154.0, last 20 avg 145.9\n",
      "Episode 131, episode return 108.0, last 20 avg 144.4\n",
      "Episode 132, episode return 126.0, last 20 avg 143.7\n",
      "Episode 133, episode return 135.0, last 20 avg 142.8\n",
      "Episode 134, episode return 145.0, last 20 avg 142.4\n",
      "Episode 135, episode return 166.0, last 20 avg 142.9\n",
      "Episode 136, episode return 109.0, last 20 avg 141.1\n",
      "Episode 137, episode return 114.0, last 20 avg 139.4\n",
      "Episode 138, episode return 132.0, last 20 avg 138.3\n",
      "Episode 139, episode return 128.0, last 20 avg 137.2\n",
      "Episode 140, episode return 138.0, last 20 avg 135.4\n",
      "Episode 141, episode return 154.0, last 20 avg 135.2\n",
      "Episode 142, episode return 248.0, last 20 avg 139.2\n",
      "Episode 143, episode return 70.0, last 20 avg 135.7\n",
      "Episode 144, episode return 130.0, last 20 avg 135.3\n",
      "Episode 145, episode return 207.0, last 20 avg 139.5\n",
      "Episode 146, episode return 161.0, last 20 avg 141.3\n",
      "Episode 147, episode return 155.0, last 20 avg 141.4\n",
      "Episode 148, episode return 194.0, last 20 avg 144.4\n",
      "Episode 149, episode return 148.0, last 20 avg 146.1\n",
      "Episode 150, episode return 130.0, last 20 avg 144.9\n",
      "Episode 151, episode return 124.0, last 20 avg 145.7\n",
      "Episode 152, episode return 119.0, last 20 avg 145.3\n",
      "Episode 153, episode return 118.0, last 20 avg 144.5\n",
      "Episode 154, episode return 111.0, last 20 avg 142.8\n",
      "Episode 155, episode return 153.0, last 20 avg 142.2\n",
      "Episode 156, episode return 141.0, last 20 avg 143.8\n",
      "Episode 157, episode return 128.0, last 20 avg 144.4\n",
      "Episode 158, episode return 135.0, last 20 avg 144.6\n",
      "Episode 159, episode return 183.0, last 20 avg 147.3\n",
      "Episode 160, episode return 151.0, last 20 avg 148.0\n",
      "Episode 161, episode return 229.0, last 20 avg 151.8\n",
      "Episode 162, episode return 145.0, last 20 avg 146.6\n",
      "Episode 163, episode return 163.0, last 20 avg 151.2\n",
      "Episode 164, episode return 175.0, last 20 avg 153.5\n",
      "Episode 165, episode return 114.0, last 20 avg 148.8\n",
      "Episode 166, episode return 116.0, last 20 avg 146.6\n",
      "Episode 167, episode return 145.0, last 20 avg 146.1\n",
      "Episode 168, episode return 121.0, last 20 avg 142.4\n",
      "Episode 169, episode return 116.0, last 20 avg 140.8\n",
      "Episode 170, episode return 107.0, last 20 avg 139.7\n",
      "Episode 171, episode return 120.0, last 20 avg 139.5\n",
      "Episode 172, episode return 132.0, last 20 avg 140.2\n",
      "Episode 173, episode return 109.0, last 20 avg 139.7\n",
      "Episode 174, episode return 140.0, last 20 avg 141.2\n",
      "Episode 175, episode return 196.0, last 20 avg 143.3\n",
      "Episode 176, episode return 115.0, last 20 avg 142.0\n",
      "Episode 177, episode return 155.0, last 20 avg 143.3\n",
      "Episode 178, episode return 126.0, last 20 avg 142.9\n",
      "Episode 179, episode return 16.0, last 20 avg 134.6\n",
      "Episode 180, episode return 99.0, last 20 avg 131.9\n",
      "Episode 181, episode return 98.0, last 20 avg 125.4\n",
      "Episode 182, episode return 100.0, last 20 avg 123.2\n",
      "Episode 183, episode return 97.0, last 20 avg 119.8\n",
      "Episode 184, episode return 99.0, last 20 avg 116.0\n",
      "Episode 185, episode return 104.0, last 20 avg 115.5\n",
      "Episode 186, episode return 102.0, last 20 avg 114.8\n",
      "Episode 187, episode return 113.0, last 20 avg 113.2\n",
      "Episode 188, episode return 111.0, last 20 avg 112.8\n",
      "Episode 189, episode return 120.0, last 20 avg 113.0\n",
      "Episode 190, episode return 163.0, last 20 avg 115.8\n",
      "Episode 191, episode return 202.0, last 20 avg 119.8\n",
      "Episode 192, episode return 131.0, last 20 avg 119.8\n",
      "Episode 193, episode return 169.0, last 20 avg 122.8\n",
      "Episode 194, episode return 15.0, last 20 avg 116.5\n",
      "Episode 195, episode return 20.0, last 20 avg 107.8\n",
      "Episode 196, episode return 27.0, last 20 avg 103.3\n",
      "Episode 197, episode return 22.0, last 20 avg 96.7\n",
      "Episode 198, episode return 59.0, last 20 avg 93.3\n",
      "Episode 199, episode return 113.0, last 20 avg 98.2\n",
      "Episode   0, episode return 23.0, last 20 avg 23.0\n",
      "Episode   1, episode return 13.0, last 20 avg 18.0\n",
      "Episode   2, episode return 14.0, last 20 avg 16.7\n",
      "Episode   3, episode return  9.0, last 20 avg 14.8\n",
      "Episode   4, episode return 11.0, last 20 avg 14.0\n",
      "Episode   5, episode return 16.0, last 20 avg 14.3\n",
      "Episode   6, episode return 17.0, last 20 avg 14.7\n",
      "Episode   7, episode return 31.0, last 20 avg 16.8\n",
      "Episode   8, episode return 12.0, last 20 avg 16.2\n",
      "Episode   9, episode return 12.0, last 20 avg 15.8\n",
      "Episode  10, episode return 14.0, last 20 avg 15.6\n",
      "Episode  11, episode return 12.0, last 20 avg 15.3\n",
      "Episode  12, episode return 14.0, last 20 avg 15.2\n",
      "Episode  13, episode return  9.0, last 20 avg 14.8\n",
      "Episode  14, episode return 16.0, last 20 avg 14.9\n",
      "Episode  15, episode return 12.0, last 20 avg 14.7\n",
      "Episode  16, episode return 15.0, last 20 avg 14.7\n",
      "Episode  17, episode return  9.0, last 20 avg 14.4\n",
      "Episode  18, episode return 15.0, last 20 avg 14.4\n",
      "Episode  19, episode return 29.0, last 20 avg 15.2\n",
      "Episode  20, episode return 17.0, last 20 avg 14.8\n",
      "Episode  21, episode return 16.0, last 20 avg 15.0\n",
      "Episode  22, episode return  8.0, last 20 avg 14.7\n",
      "Episode  23, episode return 25.0, last 20 avg 15.5\n",
      "Episode  24, episode return 15.0, last 20 avg 15.7\n",
      "Episode  25, episode return 13.0, last 20 avg 15.6\n",
      "Episode  26, episode return 12.0, last 20 avg 15.3\n",
      "Episode  27, episode return 14.0, last 20 avg 14.4\n",
      "Episode  28, episode return 11.0, last 20 avg 14.4\n",
      "Episode  29, episode return 14.0, last 20 avg 14.5\n",
      "Episode  30, episode return 13.0, last 20 avg 14.4\n",
      "Episode  31, episode return 14.0, last 20 avg 14.6\n",
      "Episode  32, episode return 17.0, last 20 avg 14.7\n",
      "Episode  33, episode return 13.0, last 20 avg 14.9\n",
      "Episode  34, episode return 11.0, last 20 avg 14.7\n",
      "Episode  35, episode return 15.0, last 20 avg 14.8\n",
      "Episode  36, episode return 11.0, last 20 avg 14.6\n",
      "Episode  37, episode return 10.0, last 20 avg 14.7\n",
      "Episode  38, episode return 14.0, last 20 avg 14.6\n",
      "Episode  39, episode return  9.0, last 20 avg 13.6\n",
      "Episode  40, episode return 23.0, last 20 avg 13.9\n",
      "Episode  41, episode return 13.0, last 20 avg 13.8\n",
      "Episode  42, episode return 40.0, last 20 avg 15.3\n",
      "Episode  43, episode return 23.0, last 20 avg 15.2\n",
      "Episode  44, episode return 19.0, last 20 avg 15.4\n",
      "Episode  45, episode return 56.0, last 20 avg 17.6\n",
      "Episode  46, episode return 54.0, last 20 avg 19.7\n",
      "Episode  47, episode return 123.0, last 20 avg 25.1\n",
      "Episode  48, episode return 69.0, last 20 avg 28.1\n",
      "Episode  49, episode return 39.0, last 20 avg 29.3\n",
      "Episode  50, episode return 48.0, last 20 avg 31.1\n",
      "Episode  51, episode return 69.0, last 20 avg 33.8\n",
      "Episode  52, episode return 40.0, last 20 avg 35.0\n",
      "Episode  53, episode return 85.0, last 20 avg 38.5\n",
      "Episode  54, episode return 89.0, last 20 avg 42.5\n",
      "Episode  55, episode return 75.0, last 20 avg 45.5\n",
      "Episode  56, episode return 33.0, last 20 avg 46.5\n",
      "Episode  57, episode return 57.0, last 20 avg 48.9\n",
      "Episode  58, episode return 90.0, last 20 avg 52.7\n",
      "Episode  59, episode return 111.0, last 20 avg 57.8\n",
      "Episode  60, episode return 73.0, last 20 avg 60.3\n",
      "Episode  61, episode return 53.0, last 20 avg 62.3\n",
      "Episode  62, episode return 68.0, last 20 avg 63.7\n",
      "Episode  63, episode return 69.0, last 20 avg 66.0\n",
      "Episode  64, episode return 53.0, last 20 avg 67.7\n",
      "Episode  65, episode return 43.0, last 20 avg 67.0\n",
      "Episode  66, episode return 93.0, last 20 avg 69.0\n",
      "Episode  67, episode return 105.0, last 20 avg 68.1\n",
      "Episode  68, episode return 123.0, last 20 avg 70.8\n",
      "Episode  69, episode return 85.0, last 20 avg 73.1\n",
      "Episode  70, episode return 56.0, last 20 avg 73.5\n",
      "Episode  71, episode return 60.0, last 20 avg 73.0\n",
      "Episode  72, episode return 55.0, last 20 avg 73.8\n",
      "Episode  73, episode return 55.0, last 20 avg 72.3\n",
      "Episode  74, episode return 51.0, last 20 avg 70.4\n",
      "Episode  75, episode return 60.0, last 20 avg 69.7\n",
      "Episode  76, episode return 66.0, last 20 avg 71.3\n",
      "Episode  77, episode return 75.0, last 20 avg 72.2\n",
      "Episode  78, episode return 62.0, last 20 avg 70.8\n",
      "Episode  79, episode return 59.0, last 20 avg 68.2\n",
      "Episode  80, episode return 98.0, last 20 avg 69.5\n",
      "Episode  81, episode return 79.0, last 20 avg 70.8\n",
      "Episode  82, episode return 101.0, last 20 avg 72.4\n",
      "Episode  83, episode return 104.0, last 20 avg 74.2\n",
      "Episode  84, episode return 143.0, last 20 avg 78.7\n",
      "Episode  85, episode return 119.0, last 20 avg 82.5\n",
      "Episode  86, episode return 209.0, last 20 avg 88.2\n",
      "Episode  87, episode return 117.0, last 20 avg 88.8\n",
      "Episode  88, episode return 112.0, last 20 avg 88.3\n",
      "Episode  89, episode return 136.0, last 20 avg 90.8\n",
      "Episode  90, episode return 89.0, last 20 avg 92.5\n",
      "Episode  91, episode return 79.0, last 20 avg 93.5\n",
      "Episode  92, episode return 85.0, last 20 avg 95.0\n",
      "Episode  93, episode return 81.0, last 20 avg 96.2\n",
      "Episode  94, episode return 84.0, last 20 avg 97.9\n",
      "Episode  95, episode return 109.0, last 20 avg 100.3\n",
      "Episode  96, episode return 127.0, last 20 avg 103.4\n",
      "Episode  97, episode return 126.0, last 20 avg 106.0\n",
      "Episode  98, episode return 189.0, last 20 avg 112.3\n",
      "Episode  99, episode return 216.0, last 20 avg 120.2\n",
      "Episode 100, episode return 99.0, last 20 avg 120.2\n",
      "Episode 101, episode return 204.0, last 20 avg 126.5\n",
      "Episode 102, episode return 106.0, last 20 avg 126.7\n",
      "Episode 103, episode return 91.0, last 20 avg 126.0\n",
      "Episode 104, episode return 304.0, last 20 avg 134.1\n",
      "Episode 105, episode return 229.0, last 20 avg 139.6\n",
      "Episode 106, episode return 136.0, last 20 avg 135.9\n",
      "Episode 107, episode return 221.0, last 20 avg 141.2\n",
      "Episode 108, episode return 94.0, last 20 avg 140.2\n",
      "Episode 109, episode return 201.0, last 20 avg 143.5\n",
      "Episode 110, episode return 108.0, last 20 avg 144.4\n",
      "Episode 111, episode return 233.0, last 20 avg 152.2\n",
      "Episode 112, episode return 185.0, last 20 avg 157.2\n",
      "Episode 113, episode return 122.0, last 20 avg 159.2\n",
      "Episode 114, episode return 118.0, last 20 avg 160.9\n",
      "Episode 115, episode return 124.0, last 20 avg 161.7\n",
      "Episode 116, episode return 136.0, last 20 avg 162.1\n",
      "Episode 117, episode return 119.0, last 20 avg 161.8\n",
      "Episode 118, episode return 142.0, last 20 avg 159.4\n",
      "Episode 119, episode return 174.0, last 20 avg 157.3\n",
      "Episode 120, episode return 233.0, last 20 avg 164.0\n",
      "Episode 121, episode return 278.0, last 20 avg 167.7\n",
      "Episode 122, episode return 331.0, last 20 avg 178.9\n",
      "Episode 123, episode return 123.0, last 20 avg 180.6\n",
      "Episode 124, episode return 132.0, last 20 avg 171.9\n",
      "Episode 125, episode return 134.0, last 20 avg 167.2\n",
      "Episode 126, episode return 124.0, last 20 avg 166.6\n",
      "Episode 127, episode return 144.0, last 20 avg 162.8\n",
      "Episode 128, episode return 82.0, last 20 avg 162.2\n",
      "Episode 129, episode return 140.0, last 20 avg 159.1\n",
      "Episode 130, episode return 138.0, last 20 avg 160.6\n",
      "Episode 131, episode return 212.0, last 20 avg 159.6\n",
      "Episode 132, episode return 113.0, last 20 avg 155.9\n",
      "Episode 133, episode return 132.0, last 20 avg 156.4\n",
      "Episode 134, episode return 88.0, last 20 avg 154.9\n",
      "Episode 135, episode return 118.0, last 20 avg 154.7\n",
      "Episode 136, episode return 90.0, last 20 avg 152.3\n",
      "Episode 137, episode return 120.0, last 20 avg 152.4\n",
      "Episode 138, episode return 96.0, last 20 avg 150.1\n",
      "Episode 139, episode return 99.0, last 20 avg 146.3\n",
      "Episode 140, episode return 80.0, last 20 avg 138.7\n",
      "Episode 141, episode return 209.0, last 20 avg 135.2\n",
      "Episode 142, episode return 189.0, last 20 avg 128.2\n",
      "Episode 143, episode return 116.0, last 20 avg 127.8\n",
      "Episode 144, episode return 98.0, last 20 avg 126.1\n",
      "Episode 145, episode return 163.0, last 20 avg 127.5\n",
      "Episode 146, episode return 118.0, last 20 avg 127.2\n",
      "Episode 147, episode return 145.0, last 20 avg 127.3\n",
      "Episode 148, episode return 98.0, last 20 avg 128.1\n",
      "Episode 149, episode return 104.0, last 20 avg 126.3\n",
      "Episode 150, episode return 114.0, last 20 avg 125.1\n",
      "Episode 151, episode return 152.0, last 20 avg 122.1\n",
      "Episode 152, episode return 119.0, last 20 avg 122.4\n",
      "Episode 153, episode return 129.0, last 20 avg 122.2\n",
      "Episode 154, episode return 208.0, last 20 avg 128.2\n",
      "Episode 155, episode return 191.0, last 20 avg 131.9\n",
      "Episode 156, episode return 215.0, last 20 avg 138.2\n",
      "Episode 157, episode return 237.0, last 20 avg 144.0\n",
      "Episode 158, episode return 500.0, last 20 avg 164.2\n",
      "Episode 159, episode return 188.0, last 20 avg 168.7\n",
      "Episode 160, episode return 136.0, last 20 avg 171.4\n",
      "Episode 161, episode return 196.0, last 20 avg 170.8\n",
      "Episode 162, episode return 237.0, last 20 avg 173.2\n",
      "Episode 163, episode return 156.0, last 20 avg 175.2\n",
      "Episode 164, episode return 218.0, last 20 avg 181.2\n",
      "Episode 165, episode return 228.0, last 20 avg 184.4\n",
      "Episode 166, episode return 193.0, last 20 avg 188.2\n",
      "Episode 167, episode return 234.0, last 20 avg 192.7\n",
      "Episode 168, episode return 291.0, last 20 avg 202.3\n",
      "Episode 169, episode return 203.0, last 20 avg 207.2\n",
      "Episode 170, episode return 150.0, last 20 avg 209.1\n",
      "Episode 171, episode return 232.0, last 20 avg 213.1\n",
      "Episode 172, episode return 187.0, last 20 avg 216.4\n",
      "Episode 173, episode return 97.0, last 20 avg 214.8\n",
      "Episode 174, episode return 120.0, last 20 avg 210.4\n",
      "Episode 175, episode return 141.0, last 20 avg 207.9\n",
      "Episode 176, episode return 117.0, last 20 avg 203.1\n",
      "Episode 177, episode return 96.0, last 20 avg 196.0\n",
      "Episode 178, episode return 113.0, last 20 avg 176.7\n",
      "Episode 179, episode return 105.0, last 20 avg 172.5\n",
      "Episode 180, episode return 148.0, last 20 avg 173.1\n",
      "Episode 181, episode return 150.0, last 20 avg 170.8\n",
      "Episode 182, episode return 160.0, last 20 avg 166.9\n",
      "Episode 183, episode return 155.0, last 20 avg 166.9\n",
      "Episode 184, episode return 137.0, last 20 avg 162.8\n",
      "Episode 185, episode return 118.0, last 20 avg 157.3\n",
      "Episode 186, episode return 115.0, last 20 avg 153.4\n",
      "Episode 187, episode return 157.0, last 20 avg 149.6\n",
      "Episode 188, episode return 127.0, last 20 avg 141.4\n",
      "Episode 189, episode return 147.0, last 20 avg 138.6\n",
      "Episode 190, episode return 113.0, last 20 avg 136.8\n",
      "Episode 191, episode return 113.0, last 20 avg 130.8\n",
      "Episode 192, episode return 139.0, last 20 avg 128.4\n",
      "Episode 193, episode return 116.0, last 20 avg 129.3\n",
      "Episode 194, episode return 130.0, last 20 avg 129.8\n",
      "Episode 195, episode return 170.0, last 20 avg 131.3\n",
      "Episode 196, episode return 151.0, last 20 avg 133.0\n",
      "Episode 197, episode return 139.0, last 20 avg 135.2\n",
      "Episode 198, episode return 201.0, last 20 avg 139.6\n",
      "Episode 199, episode return 151.0, last 20 avg 141.8\n",
      "Episode   0, episode return 19.0, last 20 avg 19.0\n",
      "Episode   1, episode return 13.0, last 20 avg 16.0\n",
      "Episode   2, episode return 46.0, last 20 avg 26.0\n",
      "Episode   3, episode return 33.0, last 20 avg 27.8\n",
      "Episode   4, episode return 28.0, last 20 avg 27.8\n",
      "Episode   5, episode return 17.0, last 20 avg 26.0\n",
      "Episode   6, episode return 22.0, last 20 avg 25.4\n",
      "Episode   7, episode return 13.0, last 20 avg 23.9\n",
      "Episode   8, episode return 23.0, last 20 avg 23.8\n",
      "Episode   9, episode return 16.0, last 20 avg 23.0\n",
      "Episode  10, episode return 15.0, last 20 avg 22.3\n",
      "Episode  11, episode return 12.0, last 20 avg 21.4\n",
      "Episode  12, episode return 16.0, last 20 avg 21.0\n",
      "Episode  13, episode return 14.0, last 20 avg 20.5\n",
      "Episode  14, episode return 10.0, last 20 avg 19.8\n",
      "Episode  15, episode return 21.0, last 20 avg 19.9\n",
      "Episode  16, episode return 13.0, last 20 avg 19.5\n",
      "Episode  17, episode return 10.0, last 20 avg 18.9\n",
      "Episode  18, episode return 12.0, last 20 avg 18.6\n",
      "Episode  19, episode return 15.0, last 20 avg 18.4\n",
      "Episode  20, episode return 14.0, last 20 avg 18.1\n",
      "Episode  21, episode return 13.0, last 20 avg 18.1\n",
      "Episode  22, episode return 12.0, last 20 avg 16.4\n",
      "Episode  23, episode return 12.0, last 20 avg 15.4\n",
      "Episode  24, episode return 11.0, last 20 avg 14.6\n",
      "Episode  25, episode return 20.0, last 20 avg 14.7\n",
      "Episode  26, episode return 20.0, last 20 avg 14.6\n",
      "Episode  27, episode return 12.0, last 20 avg 14.6\n",
      "Episode  28, episode return 10.0, last 20 avg 13.9\n",
      "Episode  29, episode return 14.0, last 20 avg 13.8\n",
      "Episode  30, episode return 11.0, last 20 avg 13.6\n",
      "Episode  31, episode return 29.0, last 20 avg 14.4\n",
      "Episode  32, episode return 41.0, last 20 avg 15.7\n",
      "Episode  33, episode return 20.0, last 20 avg 16.0\n",
      "Episode  34, episode return 12.0, last 20 avg 16.1\n",
      "Episode  35, episode return 14.0, last 20 avg 15.8\n",
      "Episode  36, episode return 14.0, last 20 avg 15.8\n",
      "Episode  37, episode return 14.0, last 20 avg 16.0\n",
      "Episode  38, episode return 24.0, last 20 avg 16.6\n",
      "Episode  39, episode return 20.0, last 20 avg 16.9\n",
      "Episode  40, episode return 18.0, last 20 avg 17.1\n",
      "Episode  41, episode return 15.0, last 20 avg 17.1\n",
      "Episode  42, episode return 19.0, last 20 avg 17.5\n",
      "Episode  43, episode return 24.0, last 20 avg 18.1\n",
      "Episode  44, episode return 14.0, last 20 avg 18.2\n",
      "Episode  45, episode return 41.0, last 20 avg 19.3\n",
      "Episode  46, episode return 18.0, last 20 avg 19.2\n",
      "Episode  47, episode return 32.0, last 20 avg 20.2\n",
      "Episode  48, episode return 48.0, last 20 avg 22.1\n",
      "Episode  49, episode return 42.0, last 20 avg 23.5\n",
      "Episode  50, episode return 55.0, last 20 avg 25.7\n",
      "Episode  51, episode return 113.0, last 20 avg 29.9\n",
      "Episode  52, episode return 43.0, last 20 avg 30.0\n",
      "Episode  53, episode return 53.0, last 20 avg 31.6\n",
      "Episode  54, episode return 34.0, last 20 avg 32.8\n",
      "Episode  55, episode return 39.0, last 20 avg 34.0\n",
      "Episode  56, episode return 62.0, last 20 avg 36.4\n",
      "Episode  57, episode return 43.0, last 20 avg 37.9\n",
      "Episode  58, episode return 50.0, last 20 avg 39.1\n",
      "Episode  59, episode return 47.0, last 20 avg 40.5\n",
      "Episode  60, episode return 54.0, last 20 avg 42.3\n",
      "Episode  61, episode return 88.0, last 20 avg 46.0\n",
      "Episode  62, episode return 115.0, last 20 avg 50.8\n",
      "Episode  63, episode return 94.0, last 20 avg 54.2\n",
      "Episode  64, episode return 89.0, last 20 avg 58.0\n",
      "Episode  65, episode return 150.0, last 20 avg 63.5\n",
      "Episode  66, episode return 17.0, last 20 avg 63.4\n",
      "Episode  67, episode return 29.0, last 20 avg 63.2\n",
      "Episode  68, episode return 24.0, last 20 avg 62.0\n",
      "Episode  69, episode return 44.0, last 20 avg 62.1\n",
      "Episode  70, episode return 54.0, last 20 avg 62.1\n",
      "Episode  71, episode return 43.0, last 20 avg 58.6\n",
      "Episode  72, episode return 22.0, last 20 avg 57.5\n",
      "Episode  73, episode return 32.0, last 20 avg 56.5\n",
      "Episode  74, episode return 49.0, last 20 avg 57.2\n",
      "Episode  75, episode return 49.0, last 20 avg 57.8\n",
      "Episode  76, episode return 39.0, last 20 avg 56.6\n",
      "Episode  77, episode return 45.0, last 20 avg 56.7\n",
      "Episode  78, episode return 63.0, last 20 avg 57.4\n",
      "Episode  79, episode return 62.0, last 20 avg 58.1\n",
      "Episode  80, episode return 72.0, last 20 avg 59.0\n",
      "Episode  81, episode return 47.0, last 20 avg 57.0\n",
      "Episode  82, episode return 14.0, last 20 avg 51.9\n",
      "Episode  83, episode return 62.0, last 20 avg 50.3\n",
      "Episode  84, episode return 76.0, last 20 avg 49.6\n",
      "Episode  85, episode return 54.0, last 20 avg 44.9\n",
      "Episode  86, episode return 52.0, last 20 avg 46.6\n",
      "Episode  87, episode return 54.0, last 20 avg 47.9\n",
      "Episode  88, episode return 68.0, last 20 avg 50.0\n",
      "Episode  89, episode return 61.0, last 20 avg 50.9\n",
      "Episode  90, episode return 79.0, last 20 avg 52.1\n",
      "Episode  91, episode return 65.0, last 20 avg 53.2\n",
      "Episode  92, episode return 66.0, last 20 avg 55.5\n",
      "Episode  93, episode return 68.0, last 20 avg 57.2\n",
      "Episode  94, episode return 100.0, last 20 avg 59.8\n",
      "Episode  95, episode return 255.0, last 20 avg 70.1\n",
      "Episode  96, episode return 163.0, last 20 avg 76.3\n",
      "Episode  97, episode return 110.0, last 20 avg 79.5\n",
      "Episode  98, episode return 105.0, last 20 avg 81.7\n",
      "Episode  99, episode return 112.0, last 20 avg 84.2\n",
      "Episode 100, episode return 109.0, last 20 avg 86.0\n",
      "Episode 101, episode return 116.0, last 20 avg 89.5\n",
      "Episode 102, episode return 134.0, last 20 avg 95.5\n",
      "Episode 103, episode return 141.0, last 20 avg 99.4\n",
      "Episode 104, episode return 121.0, last 20 avg 101.7\n",
      "Episode 105, episode return 116.0, last 20 avg 104.8\n",
      "Episode 106, episode return 78.0, last 20 avg 106.0\n",
      "Episode 107, episode return 126.0, last 20 avg 109.7\n",
      "Episode 108, episode return 162.0, last 20 avg 114.3\n",
      "Episode 109, episode return 153.0, last 20 avg 119.0\n",
      "Episode 110, episode return 163.0, last 20 avg 123.2\n",
      "Episode 111, episode return 118.0, last 20 avg 125.8\n",
      "Episode 112, episode return 127.0, last 20 avg 128.8\n",
      "Episode 113, episode return 142.0, last 20 avg 132.6\n",
      "Episode 114, episode return 147.0, last 20 avg 134.9\n",
      "Episode 115, episode return 117.0, last 20 avg 128.0\n",
      "Episode 116, episode return 128.0, last 20 avg 126.2\n",
      "Episode 117, episode return 135.0, last 20 avg 127.5\n",
      "Episode 118, episode return 128.0, last 20 avg 128.7\n",
      "Episode 119, episode return 135.0, last 20 avg 129.8\n",
      "Episode 120, episode return 165.0, last 20 avg 132.6\n",
      "Episode 121, episode return 130.0, last 20 avg 133.3\n",
      "Episode 122, episode return 278.0, last 20 avg 140.5\n",
      "Episode 123, episode return 341.0, last 20 avg 150.5\n",
      "Episode 124, episode return 270.0, last 20 avg 157.9\n",
      "Episode 125, episode return 223.0, last 20 avg 163.3\n",
      "Episode 126, episode return 176.0, last 20 avg 168.2\n",
      "Episode 127, episode return 205.0, last 20 avg 172.2\n",
      "Episode 128, episode return 131.0, last 20 avg 170.6\n",
      "Episode 129, episode return 148.0, last 20 avg 170.3\n",
      "Episode 130, episode return 176.0, last 20 avg 171.0\n",
      "Episode 131, episode return 166.0, last 20 avg 173.4\n",
      "Episode 132, episode return 162.0, last 20 avg 175.2\n",
      "Episode 133, episode return 166.0, last 20 avg 176.3\n",
      "Episode 134, episode return 178.0, last 20 avg 177.9\n",
      "Episode 135, episode return 181.0, last 20 avg 181.1\n",
      "Episode 136, episode return 195.0, last 20 avg 184.4\n",
      "Episode 137, episode return 138.0, last 20 avg 184.6\n",
      "Episode 138, episode return 168.0, last 20 avg 186.6\n",
      "Episode 139, episode return 143.0, last 20 avg 187.0\n",
      "Episode 140, episode return 263.0, last 20 avg 191.9\n",
      "Episode 141, episode return 210.0, last 20 avg 195.9\n",
      "Episode 142, episode return 208.0, last 20 avg 192.4\n",
      "Episode 143, episode return 180.0, last 20 avg 184.3\n",
      "Episode 144, episode return 166.0, last 20 avg 179.2\n",
      "Episode 145, episode return 200.0, last 20 avg 178.0\n",
      "Episode 146, episode return 201.0, last 20 avg 179.2\n",
      "Episode 147, episode return 174.0, last 20 avg 177.7\n",
      "Episode 148, episode return 120.0, last 20 avg 177.2\n",
      "Episode 149, episode return 125.0, last 20 avg 176.0\n",
      "Episode 150, episode return 133.0, last 20 avg 173.8\n",
      "Episode 151, episode return 144.0, last 20 avg 172.8\n",
      "Episode 152, episode return 152.0, last 20 avg 172.2\n",
      "Episode 153, episode return 159.0, last 20 avg 171.9\n",
      "Episode 154, episode return 144.0, last 20 avg 170.2\n",
      "Episode 155, episode return 130.0, last 20 avg 167.7\n",
      "Episode 156, episode return 125.0, last 20 avg 164.2\n",
      "Episode 157, episode return 122.0, last 20 avg 163.3\n",
      "Episode 158, episode return 148.0, last 20 avg 162.3\n",
      "Episode 159, episode return 121.0, last 20 avg 161.2\n",
      "Episode 160, episode return 108.0, last 20 avg 153.5\n",
      "Episode 161, episode return 101.0, last 20 avg 148.1\n",
      "Episode 162, episode return 126.0, last 20 avg 143.9\n",
      "Episode 163, episode return 124.0, last 20 avg 141.2\n",
      "Episode 164, episode return 112.0, last 20 avg 138.4\n",
      "Episode 165, episode return 121.0, last 20 avg 134.5\n",
      "Episode 166, episode return 146.0, last 20 avg 131.8\n",
      "Episode 167, episode return 126.0, last 20 avg 129.3\n",
      "Episode 168, episode return 153.0, last 20 avg 131.0\n",
      "Episode 169, episode return 159.0, last 20 avg 132.7\n",
      "Episode 170, episode return 144.0, last 20 avg 133.2\n",
      "Episode 171, episode return 137.0, last 20 avg 132.9\n",
      "Episode 172, episode return 180.0, last 20 avg 134.3\n",
      "Episode 173, episode return 244.0, last 20 avg 138.6\n",
      "Episode 174, episode return 208.0, last 20 avg 141.8\n",
      "Episode 175, episode return 96.0, last 20 avg 140.1\n",
      "Episode 176, episode return 212.0, last 20 avg 144.4\n",
      "Episode 177, episode return 165.0, last 20 avg 146.6\n",
      "Episode 178, episode return 227.0, last 20 avg 150.5\n",
      "Episode 179, episode return 180.0, last 20 avg 153.4\n",
      "Episode 180, episode return 166.0, last 20 avg 156.3\n",
      "Episode 181, episode return 133.0, last 20 avg 157.9\n",
      "Episode 182, episode return 101.0, last 20 avg 156.7\n",
      "Episode 183, episode return 125.0, last 20 avg 156.8\n",
      "Episode 184, episode return 120.0, last 20 avg 157.2\n",
      "Episode 185, episode return 154.0, last 20 avg 158.8\n",
      "Episode 186, episode return 210.0, last 20 avg 162.0\n",
      "Episode 187, episode return 179.0, last 20 avg 164.7\n",
      "Episode 188, episode return 276.0, last 20 avg 170.8\n",
      "Episode 189, episode return 203.0, last 20 avg 173.0\n",
      "Episode 190, episode return 305.0, last 20 avg 181.1\n",
      "Episode 191, episode return 119.0, last 20 avg 180.2\n",
      "Episode 192, episode return 123.0, last 20 avg 177.3\n",
      "Episode 193, episode return 220.0, last 20 avg 176.1\n",
      "Episode 194, episode return 123.0, last 20 avg 171.8\n",
      "Episode 195, episode return 257.0, last 20 avg 179.9\n",
      "Episode 196, episode return 220.0, last 20 avg 180.3\n",
      "Episode 197, episode return 123.0, last 20 avg 178.2\n",
      "Episode 198, episode return 183.0, last 20 avg 176.0\n",
      "Episode 199, episode return 236.0, last 20 avg 178.8\n",
      "Episode   0, episode return 18.0, last 20 avg 18.0\n",
      "Episode   1, episode return 17.0, last 20 avg 17.5\n",
      "Episode   2, episode return 30.0, last 20 avg 21.7\n",
      "Episode   3, episode return 30.0, last 20 avg 23.8\n",
      "Episode   4, episode return 22.0, last 20 avg 23.4\n",
      "Episode   5, episode return 49.0, last 20 avg 27.7\n",
      "Episode   6, episode return 19.0, last 20 avg 26.4\n",
      "Episode   7, episode return 13.0, last 20 avg 24.8\n",
      "Episode   8, episode return 14.0, last 20 avg 23.6\n",
      "Episode   9, episode return 24.0, last 20 avg 23.6\n",
      "Episode  10, episode return 13.0, last 20 avg 22.6\n",
      "Episode  11, episode return 13.0, last 20 avg 21.8\n",
      "Episode  12, episode return 17.0, last 20 avg 21.5\n",
      "Episode  13, episode return 22.0, last 20 avg 21.5\n",
      "Episode  14, episode return 17.0, last 20 avg 21.2\n",
      "Episode  15, episode return 15.0, last 20 avg 20.8\n",
      "Episode  16, episode return 20.0, last 20 avg 20.8\n",
      "Episode  17, episode return  9.0, last 20 avg 20.1\n",
      "Episode  18, episode return 15.0, last 20 avg 19.8\n",
      "Episode  19, episode return 12.0, last 20 avg 19.4\n",
      "Episode  20, episode return 14.0, last 20 avg 19.2\n",
      "Episode  21, episode return 45.0, last 20 avg 20.6\n",
      "Episode  22, episode return 34.0, last 20 avg 20.9\n",
      "Episode  23, episode return 27.0, last 20 avg 20.7\n",
      "Episode  24, episode return 57.0, last 20 avg 22.4\n",
      "Episode  25, episode return 32.0, last 20 avg 21.6\n",
      "Episode  26, episode return 23.0, last 20 avg 21.8\n",
      "Episode  27, episode return 82.0, last 20 avg 25.2\n",
      "Episode  28, episode return 52.0, last 20 avg 27.1\n",
      "Episode  29, episode return 91.0, last 20 avg 30.5\n",
      "Episode  30, episode return 115.0, last 20 avg 35.6\n",
      "Episode  31, episode return 43.0, last 20 avg 37.1\n",
      "Episode  32, episode return 202.0, last 20 avg 46.4\n",
      "Episode  33, episode return 121.0, last 20 avg 51.3\n",
      "Episode  34, episode return 102.0, last 20 avg 55.5\n",
      "Episode  35, episode return 119.0, last 20 avg 60.8\n",
      "Episode  36, episode return 92.0, last 20 avg 64.3\n",
      "Episode  37, episode return 111.0, last 20 avg 69.5\n",
      "Episode  38, episode return 115.0, last 20 avg 74.5\n",
      "Episode  39, episode return 115.0, last 20 avg 79.6\n",
      "Episode  40, episode return 95.0, last 20 avg 83.7\n",
      "Episode  41, episode return 106.0, last 20 avg 86.7\n",
      "Episode  42, episode return 154.0, last 20 avg 92.7\n",
      "Episode  43, episode return 392.0, last 20 avg 111.0\n",
      "Episode  44, episode return 166.0, last 20 avg 116.4\n",
      "Episode  45, episode return 208.0, last 20 avg 125.2\n",
      "Episode  46, episode return 132.0, last 20 avg 130.7\n",
      "Episode  47, episode return 124.0, last 20 avg 132.8\n",
      "Episode  48, episode return 123.0, last 20 avg 136.3\n",
      "Episode  49, episode return 121.0, last 20 avg 137.8\n",
      "Episode  50, episode return 120.0, last 20 avg 138.1\n",
      "Episode  51, episode return 134.0, last 20 avg 142.6\n",
      "Episode  52, episode return 133.0, last 20 avg 139.2\n",
      "Episode  53, episode return 182.0, last 20 avg 142.2\n",
      "Episode  54, episode return 107.0, last 20 avg 142.4\n",
      "Episode  55, episode return 94.0, last 20 avg 141.2\n",
      "Episode  56, episode return 228.0, last 20 avg 148.0\n",
      "Episode  57, episode return 159.0, last 20 avg 150.4\n",
      "Episode  58, episode return 164.0, last 20 avg 152.8\n",
      "Episode  59, episode return 216.0, last 20 avg 157.9\n",
      "Episode  60, episode return 265.0, last 20 avg 166.4\n",
      "Episode  61, episode return 185.0, last 20 avg 170.3\n",
      "Episode  62, episode return 206.0, last 20 avg 172.9\n",
      "Episode  63, episode return 157.0, last 20 avg 161.2\n",
      "Episode  64, episode return 151.0, last 20 avg 160.4\n",
      "Episode  65, episode return 276.0, last 20 avg 163.8\n",
      "Episode  66, episode return 308.0, last 20 avg 172.7\n",
      "Episode  67, episode return 200.0, last 20 avg 176.4\n",
      "Episode  68, episode return 187.0, last 20 avg 179.7\n",
      "Episode  69, episode return 201.0, last 20 avg 183.7\n",
      "Episode  70, episode return 174.0, last 20 avg 186.3\n",
      "Episode  71, episode return 248.0, last 20 avg 192.1\n",
      "Episode  72, episode return 180.0, last 20 avg 194.4\n",
      "Episode  73, episode return 270.0, last 20 avg 198.8\n",
      "Episode  74, episode return 386.0, last 20 avg 212.8\n",
      "Episode  75, episode return 127.0, last 20 avg 214.4\n",
      "Episode  76, episode return 110.0, last 20 avg 208.5\n",
      "Episode  77, episode return 128.0, last 20 avg 206.9\n",
      "Episode  78, episode return 134.0, last 20 avg 205.4\n",
      "Episode  79, episode return 133.0, last 20 avg 201.3\n",
      "Episode  80, episode return 157.0, last 20 avg 195.9\n",
      "Episode  81, episode return 146.0, last 20 avg 193.9\n",
      "Episode  82, episode return 180.0, last 20 avg 192.7\n",
      "Episode  83, episode return 146.0, last 20 avg 192.1\n",
      "Episode  84, episode return 166.0, last 20 avg 192.8\n",
      "Episode  85, episode return 368.0, last 20 avg 197.4\n",
      "Episode  86, episode return 133.0, last 20 avg 188.7\n",
      "Episode  87, episode return 115.0, last 20 avg 184.4\n",
      "Episode  88, episode return 128.0, last 20 avg 181.5\n",
      "Episode  89, episode return 216.0, last 20 avg 182.2\n",
      "Episode  90, episode return 144.0, last 20 avg 180.8\n",
      "Episode  91, episode return 143.0, last 20 avg 175.5\n",
      "Episode  92, episode return 142.0, last 20 avg 173.6\n",
      "Episode  93, episode return 127.0, last 20 avg 166.4\n",
      "Episode  94, episode return 139.0, last 20 avg 154.1\n",
      "Episode  95, episode return 450.0, last 20 avg 170.2\n",
      "Episode  96, episode return 175.0, last 20 avg 173.5\n",
      "Episode  97, episode return 203.0, last 20 avg 177.2\n",
      "Episode  98, episode return 129.0, last 20 avg 177.0\n",
      "Episode  99, episode return 120.0, last 20 avg 176.3\n",
      "Episode 100, episode return 125.0, last 20 avg 174.8\n",
      "Episode 101, episode return 20.0, last 20 avg 168.4\n",
      "Episode 102, episode return 43.0, last 20 avg 161.6\n",
      "Episode 103, episode return 100.0, last 20 avg 159.3\n",
      "Episode 104, episode return 112.0, last 20 avg 156.6\n",
      "Episode 105, episode return 178.0, last 20 avg 147.1\n",
      "Episode 106, episode return 224.0, last 20 avg 151.7\n",
      "Episode 107, episode return 286.0, last 20 avg 160.2\n",
      "Episode 108, episode return 139.0, last 20 avg 160.8\n",
      "Episode 109, episode return 111.0, last 20 avg 155.5\n",
      "Episode 110, episode return 109.0, last 20 avg 153.8\n",
      "Episode 111, episode return 120.0, last 20 avg 152.6\n",
      "Episode 112, episode return 149.0, last 20 avg 152.9\n",
      "Episode 113, episode return 184.0, last 20 avg 155.8\n",
      "Episode 114, episode return 240.0, last 20 avg 160.8\n",
      "Episode 115, episode return 210.0, last 20 avg 148.8\n",
      "Episode 116, episode return 156.0, last 20 avg 147.9\n",
      "Episode 117, episode return 154.0, last 20 avg 145.4\n",
      "Episode 118, episode return 131.0, last 20 avg 145.6\n",
      "Episode 119, episode return 227.0, last 20 avg 150.9\n",
      "Episode 120, episode return 216.0, last 20 avg 155.4\n",
      "Episode 121, episode return 135.0, last 20 avg 161.2\n",
      "Episode 122, episode return 106.0, last 20 avg 164.3\n",
      "Episode 123, episode return 156.0, last 20 avg 167.2\n",
      "Episode 124, episode return 279.0, last 20 avg 175.5\n",
      "Episode 125, episode return 210.0, last 20 avg 177.1\n",
      "Episode 126, episode return 173.0, last 20 avg 174.6\n",
      "Episode 127, episode return 237.0, last 20 avg 172.1\n",
      "Episode 128, episode return 138.0, last 20 avg 172.1\n",
      "Episode 129, episode return 121.0, last 20 avg 172.6\n",
      "Episode 130, episode return 227.0, last 20 avg 178.4\n",
      "Episode 131, episode return 136.0, last 20 avg 179.2\n",
      "Episode 132, episode return 161.0, last 20 avg 179.8\n",
      "Episode 133, episode return 204.0, last 20 avg 180.8\n",
      "Episode 134, episode return 134.0, last 20 avg 175.6\n",
      "Episode 135, episode return 210.0, last 20 avg 175.6\n",
      "Episode 136, episode return 157.0, last 20 avg 175.6\n",
      "Episode 137, episode return 159.0, last 20 avg 175.8\n",
      "Episode 138, episode return 108.0, last 20 avg 174.7\n",
      "Episode 139, episode return 168.0, last 20 avg 171.8\n",
      "Episode 140, episode return 283.0, last 20 avg 175.1\n",
      "Episode 141, episode return 142.0, last 20 avg 175.4\n",
      "Episode 142, episode return 220.0, last 20 avg 181.2\n",
      "Episode 143, episode return 139.0, last 20 avg 180.3\n",
      "Episode 144, episode return 162.0, last 20 avg 174.4\n",
      "Episode 145, episode return 14.0, last 20 avg 164.7\n",
      "Episode 146, episode return 22.0, last 20 avg 157.1\n",
      "Episode 147, episode return 17.0, last 20 avg 146.1\n",
      "Episode 148, episode return 15.0, last 20 avg 139.9\n",
      "Episode 149, episode return 32.0, last 20 avg 135.5\n",
      "Episode 150, episode return 118.0, last 20 avg 130.1\n",
      "Episode 151, episode return 152.0, last 20 avg 130.8\n",
      "Episode 152, episode return 125.0, last 20 avg 129.1\n",
      "Episode 153, episode return 200.0, last 20 avg 128.8\n",
      "Episode 154, episode return 273.0, last 20 avg 135.8\n",
      "Episode 155, episode return 169.0, last 20 avg 133.8\n",
      "Episode 156, episode return 168.0, last 20 avg 134.3\n",
      "Episode 157, episode return 29.0, last 20 avg 127.8\n",
      "Episode 158, episode return 134.0, last 20 avg 129.1\n",
      "Episode 159, episode return 121.0, last 20 avg 126.8\n",
      "Episode 160, episode return 125.0, last 20 avg 118.8\n",
      "Episode 161, episode return 137.0, last 20 avg 118.6\n",
      "Episode 162, episode return 177.0, last 20 avg 116.5\n",
      "Episode 163, episode return 132.0, last 20 avg 116.1\n",
      "Episode 164, episode return 139.0, last 20 avg 115.0\n",
      "Episode 165, episode return 37.0, last 20 avg 116.1\n",
      "Episode 166, episode return 168.0, last 20 avg 123.4\n",
      "Episode 167, episode return 111.0, last 20 avg 128.1\n",
      "Episode 168, episode return 163.0, last 20 avg 135.5\n",
      "Episode 169, episode return 148.0, last 20 avg 141.3\n",
      "Episode 170, episode return 154.0, last 20 avg 143.1\n",
      "Episode 171, episode return 123.0, last 20 avg 141.7\n",
      "Episode 172, episode return 87.0, last 20 avg 139.8\n",
      "Episode 173, episode return 123.0, last 20 avg 135.9\n",
      "Episode 174, episode return 120.0, last 20 avg 128.2\n",
      "Episode 175, episode return 114.0, last 20 avg 125.5\n",
      "Episode 176, episode return 131.0, last 20 avg 123.7\n",
      "Episode 177, episode return 141.0, last 20 avg 129.2\n",
      "Episode 178, episode return 207.0, last 20 avg 132.9\n",
      "Episode 179, episode return 148.0, last 20 avg 134.2\n",
      "Episode 180, episode return 138.0, last 20 avg 134.9\n",
      "Episode 181, episode return 198.0, last 20 avg 137.9\n",
      "Episode 182, episode return 114.0, last 20 avg 134.8\n",
      "Episode 183, episode return 228.0, last 20 avg 139.6\n",
      "Episode 184, episode return 136.0, last 20 avg 139.4\n",
      "Episode 185, episode return 13.0, last 20 avg 138.2\n",
      "Episode 186, episode return 14.0, last 20 avg 130.6\n",
      "Episode 187, episode return 39.0, last 20 avg 127.0\n",
      "Episode 188, episode return 48.0, last 20 avg 121.2\n",
      "Episode 189, episode return 15.0, last 20 avg 114.5\n",
      "Episode 190, episode return 37.0, last 20 avg 108.7\n",
      "Episode 191, episode return 25.0, last 20 avg 103.8\n",
      "Episode 192, episode return 131.0, last 20 avg 106.0\n",
      "Episode 193, episode return 122.0, last 20 avg 106.0\n",
      "Episode 194, episode return 132.0, last 20 avg 106.5\n",
      "Episode 195, episode return 182.0, last 20 avg 110.0\n",
      "Episode 196, episode return 116.0, last 20 avg 109.2\n",
      "Episode 197, episode return 129.0, last 20 avg 108.6\n",
      "Episode 198, episode return 107.0, last 20 avg 103.6\n",
      "Episode 199, episode return 108.0, last 20 avg 101.6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = gym.make(\"CartPole-v1\")\n",
    "nb_seeds = 4\n",
    "\n",
    "\n",
    "seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    # Here, don't forget to reset the agent at each seeds!\n",
    "    agent = DQNAgent(environment.observation_space, environment.action_space)\n",
    "    seeds_result.append(simulation(environment, agent))\n",
    "\n",
    "seeds_result = np.array(seeds_result)\n",
    "means = np.mean(seeds_result, axis=0)\n",
    "stds = np.std(seeds_result, axis=0)\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(means, color=\"g\")\n",
    "plt.fill_between([x for x in range(len(means))], means + stds, means - stds, color=\"g\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b47bdf",
   "metadata": {},
   "source": [
    "## Actor-critic\n",
    "\n",
    "<div id=\"actor_critic\"></div>\n",
    "\n",
    "As we saw earlier in this class, Deep Q-Network (DQN) was made for discrete actions spaces. \n",
    "It use a Q-network, that is made to give the expected value of each action in a given state.\n",
    "In continuous action space, this algorithm cannot be used because we cannot allocate one neural-network output for each action. \n",
    "To correct this problem, the main idea is to make the Q-Network return a single outpur, that will be the value of the state action pair, that is given in input. But do this prevent us to choose the action, and that's why we add another neural network called the actor, taking a state as the input, and returning the value of each continuous action to take (1 action between -1 and 1 for pendulum, so 1 neuron at the output, 2 for lunar lander).\n",
    "\n",
    "To train this neural network, we just have to use the Q-Network, to estimate how much the chosen action is good to take in the given state.\n",
    "\n",
    "This is the main idea around actor critic architecture. To understand in detail how it work, YOU are going to implement two of the most known off-policy actor critic architectures, Deep-Deterministic Policy Gradient (DDPG) and Soft Actor-Critic (SAC). \n",
    "\n",
    "As in other notebook, the code of these two algorithms are given. But because the number of questions/task to do is low in this notebook compared to others, I hardly recomend you to code them by yourself first. Try to run them, and if it's not working well, try some improvement or to understand what is not working, before to give up and look the solution. You will have the entire session to implement 2 algorithms, so take your time to make sure you understood them well.\n",
    "\n",
    "### Deep-Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "<div id=\"ddpg\"></div>\n",
    "\n",
    "DDPG is considered as an off-policy algorithm, because the policy used to train the critic is different than the one used to generate samples.\n",
    "\n",
    "For the general idea, when we select an interaction (s, a, r, s', a') to train our critic network, the way a' will be chosen will define the reward propagation (generally called credit assignment) speed. If a' is the optimal action from s', like in DQN, there is a high probability to get an action that lead in interestig and highly valuated states s''. In that case, the high values and the rewards will be propagated to (s, a) value, more frequently than s'' with low value.\n",
    "\n",
    "In SARSA case, the a' actions will be chose following GLIE actor (cf RL1 - RL fundamentals notebook), that will depend on an exploration strategy, so a' can be random sometime. When a' is random, we have a high probability to evaluate Q'(s', a') where action a' is a bad action in state s'. In this case, Q(s, a) will also be learned as being a bad action state pair, even if s' can maybe lead to an interesting reward. We call SARSA an on-policy algorithm. For a purely actor-critic on-policy algorithm, you can also check [A3C](https://arxiv.org/pdf/1602.01783.pdf) but you don't need to understand this algorithm in this notebook.\n",
    "\n",
    "If you didn't got it, you can also check this course from Olivier SIGAUD on youtube : https://youtu.be/hlhzvQnXdAA\n",
    "\n",
    "Now, I want you to implement DDPG algorithm, that use an actor network, a critic network, and two target networks for both actor and critic to respectively compute a' and Q'.\n",
    "\n",
    "Note that the default DDPG algorithm don't use epsilon-greedy, but add a noise on the action to make it explore. The original paper use a OU-noise, but for simplicity, we will generate a gaussian noise N(mean, std) to noise our action.\n",
    "\n",
    "To make sure you know how to implement it, let's answer to some questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3d016",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If we have an actor network A, a critic actor network A' and a state s, how will I compute actions a to build samples when my agent is interacting with it's environment ? How will I compute action a' to compute Q'(s', a') to train my critic network ?\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d28b6",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "When the agent is generating training samples while interacting with it's environment, he need to explore it to find new interesting states. In this case, the choosen action will be \n",
    "\n",
    "    a = A(s) + noise\n",
    "    \n",
    "When the agent is generations next_action a' to train critic by computing Q'(s', a'), we don't want him to explore so the generated a' have a higher probability to correspond to a highly valuable Q'(s', a'), so this value have a higher change to be propagated to Q(s, a), speeding up the training.\n",
    "<details/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5034d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If the critic is used to evaluate the action a taken by the actor network in a given state s. What will be the loss of the critic network ?\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab0bd1",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "In this situation, the critic is learning a Q-value, like the DQN model is doing. For this reason, the loss of our critic network should be similar:\n",
    "    \n",
    "    # Pseudo code\n",
    "    target = reward + gamma * (1 - done) * target_critic(s', a')\n",
    "    critic_loss = MSE(critic(s, a), target)\n",
    "<details/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93494b2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If the critic is able to evaluate an action a, taken by the actor network in a state s, what should be the actor loss ? (do not search for something to complicated, the answer is simple! :)\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34950c",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "Note that if we are learning a sample s, a, r, s', d from the buffer, only s can be used in the actor training. In fact, the action chosen by the actor at time step t, will not obviously be the same than the action taken by actor at time step t - n, when the sample was generated in the environment (because the actor is learning at every time step). For this reason, a, r, s', d and no more re-usable for the actor.\n",
    "\n",
    "    a = actor(s)\n",
    "    loss_actor = -critic(s, a)\n",
    "\n",
    "Note that the actor nedd the critic to be trained befor he can learn. In some way, the actor ability is trained to make shure the agent is going in states that satisfy the critic. Generally, in the learn() function, we train the critic before the actor, but is just speed-up the learning by 1 time step so it's not mandatory.\n",
    "<details/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7e751",
   "metadata": {},
   "source": [
    "#### Performances\n",
    "\n",
    "You shouldn't be worry if your DDPG training is long. In my experiments, the learning took 100 episodes to get a sum of reawards around -700 in average, and 200 episodes to converge between -200 and -400 with the following attributes:\n",
    "\n",
    " - Env: Pendulum-v0\n",
    " - Noise: mean=0, std=0.1\n",
    " - actor_learning_rate=0.000025\n",
    " - critic_learning_rate=0.00025, \n",
    " - gamma=0.99,  # Discount factor\n",
    " - buffer_max_size=1000000, \n",
    " - layer1_size=200, # For both critic and actor\n",
    " - layer2_size=150, # For both critic and actor\n",
    " - batch_size=64,  # For both critic and actor\n",
    " - tau=0.01,\n",
    " \n",
    "The last parameter tau is to update target networks. At each time steps, I update the weights of target networks to be a weighted average between their weights, and the targeted network weights :\n",
    "\n",
    "    target_model_weight = target_model_weight * (1 - tau) + model_weight * tau\n",
    "    \n",
    "This is another way to update target critic and target actor, but the one you learned in RL5 notebook (copy weights every n time steps) should also work.\n",
    "\n",
    "Here I give you an architecture for both actor and critic neural networks. The Sequential function we used before should work, but the learning in continuous action space is really long, and use LayerNorm layers increase so much learning speed, so you will gain time on your notebook. Initialise weights as I do here increase a bit more the learning speed, but according to my experience the impact is not really increadible.\n",
    "\n",
    "[WARN] I also recommand to use torch.tanh activation function on the last layer of the actor since it increase the learning.\n",
    "\n",
    "I can't explain why all of these increase the learning speed, I just observed it guided by some tips found on internet ... Everithing you should understand and remember here, is why the default architecture and behaviour of DDPG make the learning converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce34aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def init_weights(layer, bound=None):\n",
    "    if bound is None:\n",
    "        bound = 1. / np.sqrt(layer.weight.data.size()[0])\n",
    "    torch.nn.init.uniform_(layer.weight.data, -bound, bound)\n",
    "    torch.nn.init.uniform_(layer.bias.data, -bound, bound)\n",
    "\n",
    "\n",
    "class DefaultNN(nn.Module):\n",
    "    def __init__(self, learning_rate, input_dims, layer_1_dims, layer_2_dims, output_dims, device,\n",
    "                 last_activation=None):\n",
    "        super().__init__()\n",
    "        self.last_activation = last_activation\n",
    "        self.layer_1 = nn.Linear(input_dims, layer_1_dims)\n",
    "        init_weights(self.layer_1)\n",
    "        self.layer_norm_1 = nn.LayerNorm(layer_1_dims)\n",
    "\n",
    "        self.layer_2 = nn.Linear(layer_1_dims, layer_2_dims)\n",
    "        init_weights(self.layer_2)\n",
    "        self.layer_norm_2 = nn.LayerNorm(layer_2_dims)\n",
    "\n",
    "        self.layer_3 = nn.Linear(layer_2_dims, output_dims)\n",
    "        init_weights(self.layer_3, bound=0.003)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        running_output = self.layer_1(inputs)\n",
    "        running_output = self.layer_norm_1(running_output)\n",
    "        running_output = torch.nn.functional.relu(running_output)\n",
    "        running_output = self.layer_2(running_output)\n",
    "        running_output = self.layer_norm_2(running_output)\n",
    "        running_output = torch.nn.functional.relu(running_output)\n",
    "        running_output = self.layer_3(running_output)\n",
    "\n",
    "        if self.last_activation is not None:\n",
    "            running_output = self.last_activation(running_output)\n",
    "        return running_output\n",
    "\n",
    "    def converge_to(self, other_model, tau=0.01):\n",
    "        \"\"\"\n",
    "        Make the value of parameters of this model converge to one from the given model.\n",
    "        The parameter tau indicate how close our weights should be from the one of the other model.\n",
    "        self.converge_to(other_model, tau=1) is equivalent to self = copy.deepcopy(other_model).\n",
    "\n",
    "        other_model should have the same shape, dimensions, than self.\n",
    "        \"\"\"\n",
    "        for self_param, other_param in zip(self.parameters(), other_model.parameters()):\n",
    "            self_param.data.copy_(\n",
    "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24577b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pie-rl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5179/3646172041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mseeds_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mseeds_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5179/3726167246.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(environment, agent, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Ending time step process ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_torque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_torque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;31m# for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mangle_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mthdot\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \"\"\"\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             um.maximum, a, min, out=out, casting=casting, **kwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         return _clip_dep_invoke_with_casting(\n\u001b[0m\u001b[1;32m    160\u001b[0m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pie-rl/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[0;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# try to deal with broken casting rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, implement DDPG. You can take inspiration from DQN, because some behaviours (like replay buffer management) \n",
    "# are still the same here.\n",
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, state_space, action_space, device, actor_lr=0.000025, critic_lr=0.00025, tau=0.01, gamma=0.99,\n",
    "                 max_size=1000000, layer1_size=200, layer2_size=150, batch_size=64, noise_std=0.1, name=\"DDPG\"):\n",
    "        assert isinstance(action_space, gym.spaces.Box)  ### NEW: The action space is now continuous \n",
    "        super().__init__(state_space, action_space, device=device, name=name)\n",
    "        # TODO\n",
    "\n",
    "    def action(self, observation):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "# Test our agent on Pendulum-v0\n",
    "environment = gym.make(\"Pendulum-v0\")\n",
    "nb_seeds = 4\n",
    "seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    seeds_result.append(simulation(environment, agent))\n",
    "\n",
    "seeds_result = np.array(seeds_result)\n",
    "means = np.mean(seeds_result, axis=0)\n",
    "stds = np.std(seeds_result, axis=0)\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(means, color=\"g\")\n",
    "plt.fill_between([x for x in range(len(means))], means + stds, means - stds, color=\"g\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e89db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL6_DDPG.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "### NEW indicate the differences between DQN and DDPG.\n",
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, state_space, action_space, device, actor_lr=0.000025, critic_lr=0.00025, tau=0.001, gamma=0.99,\n",
    "                 max_size=1000000, layer1_size=200, layer2_size=150, batch_size=64, noise_std=0.1, name=\"DDPG\"):\n",
    "        assert isinstance(action_space, gym.spaces.Box)  ### NEW: The action space is now continuous \n",
    "        super().__init__(state_space, action_space, device=device, name=name)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(max_size, self.device)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = DefaultNN(actor_lr, self.state_size, layer1_size, layer2_size, self.nb_actions, self.device,\n",
    "                               last_activation=torch.tanh)\n",
    "        self.critic = DefaultNN(critic_lr, self.state_size + self.nb_actions, layer1_size, layer2_size, 1, self.device)\n",
    "\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.normal_distribution = torch.distributions.normal.Normal(\n",
    "            torch.zeros(self.nb_actions), torch.full((self.nb_actions,), noise_std))\n",
    "\n",
    "    def action(self, observation):\n",
    "        with torch.no_grad():\n",
    "            observation = torch.tensor(observation, dtype=torch.float).to(self.device)\n",
    "            actor_output = self.actor.forward(observation).to(self.device)\n",
    "            noise = self.normal_distribution.sample()\n",
    "            action = actor_output + noise\n",
    "        return action.cpu().detach().numpy()\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
    "        self.learn()\n",
    "        super().on_action_stop(action, new_state, reward, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            states, actions, rewards, new_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_actions = self.target_actor.forward(new_states)\n",
    "                critic_value_ = self.target_critic.forward(torch.concat((new_states, target_actions), dim=-1))\n",
    "            critic_value = self.critic.forward(torch.concat((states, actions), dim=-1))\n",
    "            target = torch.addcmul(rewards, self.gamma, 1 - dones, critic_value_.squeeze()).view(self.batch_size, 1)\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            critic_loss = torch.nn.functional.mse_loss(target, critic_value)\n",
    "            critic_loss.backward()\n",
    "            self.critic.optimizer.step()\n",
    "\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actions = self.actor.forward(states)\n",
    "            actor_loss = - self.critic.forward(torch.concat((states, actions), dim=-1))\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "\n",
    "            self.target_critic.converge_to(self.critic, tau=self.tau)\n",
    "            self.target_actor.converge_to(self.actor, tau=self.tau)\n",
    "\n",
    "# Test our agent on Pendulum-v0\n",
    "environment = gym.make(\"Pendulum-v0\")\n",
    "nb_seeds = 4\n",
    "seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    seeds_result.append(simulation(environment, agent))\n",
    "\n",
    "seeds_result = np.array(seeds_result)\n",
    "means = np.mean(seeds_result, axis=0)\n",
    "stds = np.std(seeds_result, axis=0)\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(means, color=\"g\")\n",
    "plt.fill_between([x for x in range(len(means))], means + stds, means - stds, color=\"g\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2421c7",
   "metadata": {},
   "source": [
    "### Soft Actor-critic (SAC)\n",
    "\n",
    "<div id=\"sac\"></div>\n",
    "\n",
    "Soft Actor-Critic is really close to DDPG, except that it learn using a regularised entropy on actions policy.\n",
    "\n",
    "#### Shanon entropy\n",
    "\n",
    "Let X be a random variable with a law of density p(X) satisfying the normalization and positivity requirements, we define its entropy by\n",
    "$$-\\int_{X} p(x) log (p(x))$$\n",
    "\n",
    "It allows to quantify the disorder of a random variable. The entropy is maximal when X follows an uniform distribution, and minimal when p(X) is equal to zero everywhere except in one value, which is a Dirac distribution.\n",
    "Loi uniforme : entropie très forte, loi normale : entropie faible, loi déterministe : entropie la plus faible\n",
    "\n",
    "#### Why entropy in SAC?\n",
    "\n",
    "In SAC algorithm, we not only consider the critic evaluation inside the actor loss, but also it's decision entropy.\n",
    "The loss of the agent should be high when the entropy is low, because it will invite him to explore more.\n",
    "The mode the entropy of the agent is important, the more it will explore.\n",
    "\n",
    "More, SAC use entropy to estimate next_states value. The more the entropy of the target actor is important while choosing a', the more s' will be condidered as highly valuable.\n",
    "To illustrate the intiution behing this, let's imagine an environment with a discrete state space, in a sigle line: \n",
    "\n",
    "\n",
    "|    |    |    |\n",
    "|:--:|:--:|:--:|\n",
    "| +1 | A  | +1 |\n",
    "\n",
    "In the exemple above, the agent (A) will recieve a reward of +1 at the next round, whatever the next action he will chose. Because the actor loss make him maximise critig grade and entropy, he will improve entropy when the critig grage cannot be improved anymore. In this case, the critic value will everytime be the same, whatever the action taken by the agent. For this reason, the action entropy will be maximize, so the agent will have the same probability to go in any directions.\n",
    "\n",
    "This fact bring a side effect. Because states are consider good when the entropy of the action taken from them is high, the agent will maintain itself in states where he will be sake whatever the next taken action. This behavior is also present in DDPG but is strengten here but the entropy maximisation, making our agent gain in stability (you will observe the gap in performances standard deviation latter).\n",
    "\n",
    "\n",
    "|    |    |    |\n",
    "|:--:|:--:|:--:|\n",
    "| -1 | A  | +1 |\n",
    "\n",
    "In this new case, the entropy of the action will be low because the actor loss will be lower if the critic evaluation is high, so he should choose deterministic action that leads to states with the higher value.\n",
    "\n",
    "In other words, we can say that SAC will perform a better exploration/exploitation trade-off, by takin both the advantage of a high exploration, and the advantage of a high exploitation. He will maximize exploration by mawimising entropy, but will not suffer from the incovenient of high exploration that is fall in bad state and never enstrenght the optimal trajectory the the best rewards, because he will fly away from bad rewarding states.\n",
    "\n",
    "\n",
    "#### How to use entropy in SAC?\n",
    "\n",
    "To compute an entropy, we need the actor to give use a random distribution. The most common way to archieve that, is to make the actor with 2 * nb_action outputs neurons, giving actions means and standard deviations.\n",
    "\n",
    "Because the standart deviation, it is common to make the actor return the log(std), and then get the real std with $e^{log_std}$. But we can also put the std output inside a relu().\n",
    "\n",
    "To get the action, we can sample actions from the means and standard deviations we got. Doing this will prevent us latter to retro-propagate the gradient when we will train our actor, so we should do a reparametrization trick when we want to compute actions for actor training (more explanation in the code to fill bellow).\n",
    "Sample with reparametrisation is equivalent to compute $a = \\mu + \\mathcal{N}(0,\\,1)\\ * sigma$.\n",
    "\n",
    "In general, for a given a', we maximise entropy my minimising $log (p(x))$. Looking at the entropy function, we can understand that $log(p(x))$ have a higher impact than p(x) only, because p(x) is between 0 and 1, and $|log(p(x))|$ is generally extremly high because p(x) is generally close to 0.\n",
    "\n",
    "Because our environment action space is bounded, but our normal distribution is not, we can put the actions we got inside tanh function to get some between -1 and 1, and then scale it to our environment action space. Because we bould our actions, we should do a process over our log prob as follow:\n",
    "\n",
    "        log_probs = actions_distribution.log_prob(actions)\n",
    "        log_probs -= torch.log(1 - action.pow(2) + self.min_std)\n",
    "        log_probs = log_probs.sum(dim=-1)\n",
    "        \n",
    "We are not going deeper about these mathematicals details, but if you are interested, the explanation if this is in [SAC paper](https://arxiv.org/pdf/1812.05905.pdf), appendic C (bottom of page 16).\n",
    "\n",
    "Now you should have any informations to complete the code bellow for SAC implementation! To gain some time, wome code parts are already filled.\n",
    "\n",
    "NB: Algorithm hyperparameters are given in $__init__$ function, the new alpha hyper parameter is the ratio between Q value and entropy inside critic update:\n",
    "$$V(s') = TargetCritic(s', a') - alpha * LogProb$$ where $$a', LogProg = SampleAction(s')$$\n",
    "\n",
    "Note that SAC is very stable and converge well, so you don't neet to have low learning rate and you also can use a Sequential() model (like we did with DQN) without LayerNorm layers inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ef7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent(Agent):\n",
    "    def __init__(self, state_space, action_space, device, actor_lr=0.001, critic_lr=0.001, gamma=0.98, \n",
    "                 max_size=10000, tau=0.005, layer1_size=128, layer2_size=128, batch_size=128, alpha=0.9):\n",
    "        super().__init__(state_space, action_space, device, \"SAC\")\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def converge_to(self, model, other_model, tau=None):\n",
    "        \"\"\"\n",
    "        Make the first model weights converge to the second one with a ration of tau.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        for self_param, other_param in zip(model.parameters(), other_model.parameters()):\n",
    "            self_param.data.copy_(\n",
    "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
    "            )\n",
    "\n",
    "    def sample_action(self, state, reparameterize=False, actor_network=None):\n",
    "        if actor_network is None:\n",
    "            actor_network = self.actor\n",
    "        # TODO\n",
    "        # 1. Sample an action from the given state using the given actor network. It can be the target or the \n",
    "        #    default actor depending on when this function is called\n",
    "        # 2. raparameterize is used to performed a reparametrisation trick if we want to keep the gradient \n",
    "        #    to retro-propagate it later. Use reparametrize=True when you call this function to train the actor.\n",
    "        #    if it is True, sample action using distribution.rsampl(), use sample() otherwise.\n",
    "        # 3. Compute the log_probability as explained in SAC description\n",
    "        # 4. Return both the actions taken and the log probabilities\n",
    "        pass\n",
    "\n",
    "    def action(self, state):\n",
    "        actions, _ = self.sample_action(state, reparameterize=False)\n",
    "        return actions.cpu().detach().numpy()\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            states, actions, rewards, next_states, done = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            # Training critic\n",
    "            # TODO\n",
    "\n",
    "            # Train actor\n",
    "            # TODO\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
    "        self.learn()\n",
    "        super().on_action_stop(action, new_state, reward, done)\n",
    "\n",
    "# Test our agent on LunarLanderContinuous-v2  (Don't work on Pendulum because of an unexpected bug)\n",
    "environment = gym.make(\"LunarLanderContinuous-v2\")\n",
    "nb_seeds = 4\n",
    "sac_seeds_result = []\n",
    "ddpg_seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    \n",
    "    print()\n",
    "    print(\"###################\")\n",
    "    print()\n",
    "    print(\"      SEED \" + str(seed_id))\n",
    "    print()\n",
    "    print(\"###################\")\n",
    "    \n",
    "    print()\n",
    "    print(\" > Training SAC\")\n",
    "    print()\n",
    "    agent = SACAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    sac_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
    "    \n",
    "    print()\n",
    "    print(\" > Training DDPG\")\n",
    "    print()\n",
    "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    ddpg_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
    "\n",
    "    sac_means = np.mean(np.array(sac_seeds_result), axis=0)\n",
    "    sac_stds = np.std(np.array(sac_seeds_result), axis=0)\n",
    "\n",
    "    ddpg_means = np.mean(np.array(ddpg_seeds_result), axis=0)\n",
    "    ddpg_stds = np.std(np.array(ddpg_seeds_result), axis=0)\n",
    "\n",
    "    plt.cla()\n",
    "    plt.plot(sac_means, color=\"g\", label=\"sac\")\n",
    "    plt.fill_between([x for x in range(len(sac_means))], sac_means + sac_stds, sac_means - sac_stds, color=\"g\", alpha=0.2)\n",
    "    plt.plot(ddpg_means, color=\"r\", label=\"ddpg\")\n",
    "    plt.fill_between([x for x in range(len(ddpg_means))], ddpg_means + ddpg_stds, ddpg_means - ddpg_stds, color=\"b\", alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68894e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/RL6_SAC.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935498c",
   "metadata": {},
   "source": [
    "Now you can see that entropy regularisation improve so much DDPG performances.\n",
    "\n",
    "You don't need to remeber every performance improvement tricks we used in this notebook, but you should understand how DDPG and SAC works, and what are the differences between them, and between DQN and them."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a19035501555ff2cb985ca696f0374861d10e5f44788dd8d73006224bc46cbc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
